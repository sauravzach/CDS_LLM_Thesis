{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'SyntheticDataintext/1Allornothingthinking.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Read the text file and store the sentences in a list\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSyntheticDataintext/1Allornothingthinking.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      6\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n",
      "File \u001b[1;32mc:\\Users\\saura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SyntheticDataintext/1Allornothingthinking.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Read the text file and store the sentences in a list\n",
    "with open('SyntheticDataintext/1Allornothingthinking.txt', 'r') as file:\n",
    "    sentences = file.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "c:\\Users\\saura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m stop_words\u001b[38;5;241m.\u001b[39mextend([])\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m sentence_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(\u001b[43msentences\u001b[49m,show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Has internal tokenzier\u001b[39;00m\n\u001b[0;32m     13\u001b[0m umap_model \u001b[38;5;241m=\u001b[39m UMAP(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, min_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     15\u001b[0m hdbscan_model \u001b[38;5;241m=\u001b[39m HDBSCAN(min_cluster_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m, cluster_selection_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meom\u001b[39m\u001b[38;5;124m'\u001b[39m, prediction_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,cluster_selection_epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.18\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_words.extend([])\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentence_embeddings = model.encode(sentences,show_progress_bar=True)  # Has internal tokenzier\n",
    "umap_model = UMAP(n_neighbors=25, n_components=8, min_dist=0, metric='cosine', random_state=42)\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=2, metric='cosine', cluster_selection_method='eom', prediction_data=True,cluster_selection_epsilon=0.18)\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=stop_words,min_df=2, ngram_range=(1, 3))\n",
    "topic_model = BERTopic(\n",
    "\n",
    "    language=\"english\",\n",
    "    calculate_probabilities=True,\n",
    "\n",
    "    # Pipeline models\n",
    "    embedding_model=model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "\n",
    "    # Hyperparameters\n",
    "    top_n_words=5,\n",
    "    verbose=True,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(sentences, sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a BERTopic model\n",
    "model = BERTopic()\n",
    "\n",
    "# Fit the BERTopic model on the list of sentences\n",
    "topics, _ = model.fit_transform(sentences)\n",
    "\n",
    "# Find duplicate sentences within each topic\n",
    "duplicate_sentences = {}\n",
    "for topic_id in set(topics):\n",
    "    topic_sentences = [sentences[i] for i, t in enumerate(topics) if t == topic_id]\n",
    "    # To keep track of original sentences found in the duplicates\n",
    "    seen = {}\n",
    "    for i in range(len(topic_sentences)):\n",
    "        for j in range(i + 1, len(topic_sentences)):\n",
    "            if set(topic_sentences[i].lower().split()) == set(topic_sentences[j].lower().split()):\n",
    "                if topic_sentences[i] not in seen and topic_sentences[j] not in seen:\n",
    "                    seen[topic_sentences[i]] = topic_sentences[j]  # Set first as original\n",
    "                elif topic_sentences[i] in seen:\n",
    "                    seen[topic_sentences[j]] = topic_sentences[i]\n",
    "                elif topic_sentences[j] in seen:\n",
    "                    seen[topic_sentences[i]] = topic_sentences[j]\n",
    "\n",
    "    duplicate_sentences[topic_id] = seen\n",
    "\n",
    "# Create a new DataFrame to store the sentences and duplicate tags\n",
    "df = pd.DataFrame({'Sentence': sentences, 'Duplicate': 'Original'})\n",
    "\n",
    "# Iterate over the duplicate sentences, and tag them in the DataFrame\n",
    "for topic_id, duplicates in duplicate_sentences.items():\n",
    "    for dup, orig in duplicates.items():\n",
    "        df.loc[df['Sentence'] == dup, 'Duplicate'] = f'Duplicate of {orig}'\n",
    "\n",
    "# Print the DataFrame to check results\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('Allornothingthinkingduplicates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bertopic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
